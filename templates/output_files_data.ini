;
; Default templates used to generate input scripts. Note that # cannot be used as comment in this file! Use ; instead
;

; These are the default resources requested in each step
[resources]
all_nodes = 1
all_cpus = 12
all_gpus = 1
all_time = 24

pack_nodes = 1
pack_cpus = 1
pack_gpus = 0
pack_time = 1

; These headers are used to generate bash input files
[bash]
submit_command = bash
header = #! /bin/bash
depend_string =


; These options are used to generate Slurm input files
[slurm]
submit_command = sbatch --parsable

header = #!/bin/bash
         #SBATCH --nodes=__NODES__
         #SBATCH --gres=gpu:__GPUS__
         #SBATCH --ntasks-per-node=__CPUS__
         #SBATCH --time=__TIME__:00:00
         #SBATCH --job-name=__JOBNAME__

depend_string = --dependency=afterok:{}


; These options are used to generate Torque/PBS input files
[pbs]
submit_command = qsub

equilibrate_str = #!/bin/bash
                  #PBS -l walltime=__TIME__:00:00
                  #PBS -l nodes=__NODES__:ppn=__CPUS__:gpus=__GPUS__
                  #PBS -N __JOBNAME__

depend_string = -W depend=afterok:{}


[constant_part]
runhrex = cd __MDDIR__

          if [ $(ls lambda*/lambda.gro 2>/dev/null | wc -l) -eq $(ls | egrep "lambda[0-9]+$" | wc -l) ]
          then
              echo "[INFO] Run step already done. Going on."
          else
              [ $(ls lambda*/lambda.gro 2>/dev/null | wc -l) -ne 0 ] && { echo "[ERROR] Some of the lambda windows are complete, but not all of them. Because this run uses HREX, I cannot continue"; exit 1; }
              mpirun -np $(ls | egrep "lambda[0-9]+$" | wc -l) __GMXBIN__ mdrun -v --deffnm lambda -cpi lambda.cpt -multidir $(ls -v | egrep "lambda[0-9]+$") -plumed plumed.dat -hrex -replex __HREX__ >& feprun_$(date "+%H%M%S_%d%m%Y").log || { echo "mdrun failed at line ${LINENO} "; exit -1; }
          fi

runnohrex = cd __MDDIR__

          if [ $(ls lambda*/lambda.gro 2>/dev/null | wc -l) -eq $(ls | egrep "lambda[0-9]+$" | wc -l) ]
          then
              echo "[INFO] Run step already done. Going on."
          else
              [ $(ls lambda*/lambda.gro 2>/dev/null | wc -l) -ne 0 ] && echo "[WARNING] Some of the lambda windows are complete, but not all of them. Trying to continue anyway."
              mpirun -np $(ls | egrep "lambda[0-9]+$" | wc -l) __GMXBIN__ mdrun -v --deffnm lambda -cpi lambda.cpt -multidir $(ls -v | egrep "lambda[0-9]+$") >& feprun_$(date "+%H%M%S_%d%m%Y").log || { echo "mdrun failed at line ${LINENO} "; exit -1; }

          fi

rerun = if [ -f 'lambda0/lambda.trr' ]
        then
            ext='trr'
        elif [ -f 'lambda0/lambda.xtc' ]
        then
            ext='xtc'
        else
            echo "[ERROR] Could not find trajectory file in lambda0, cannot continue"
            exit -1
        fi

        fn0_cpu_gpu () {
            gpu_array=( ${CUDA_VISIBLE_DEVICES//,/ } )
            mult=$(echo "print(int(${SLURM_CPUS_ON_NODE}/${#gpu_array[@]}) + 1)" | python -u)
            gpu_array=( $(echo "print('${gpu_array[@]} ' * ${mult})" | python -u) )
            [ ! -f rerun/rerun_struct${1}_coord${2}.edr ] && __GMXBIN__ mdrun -gpu_id ${gpu_array[${3}]} -ntomp ${n_cpus} -rerun lambda${2}/lambda.${ext} -s lambda${1}/lambda_nocont.tpr -e rerun/rerun_struct${1}_coord${2}.edr -g rerun/rerun_struct${1}_coord${2}.log >& rerun/rerun_struct${1}_coord${2}.out
        }

        fn0_cpu () {
            [ ! -f rerun/rerun_struct${1}_coord${2}.edr ] && __GMXBIN__ mdrun -ntomp ${n_cpus} -rerun lambda${2}/lambda.${ext} -s lambda${1}/lambda_nocont.tpr -e rerun/rerun_struct${1}_coord${2}.edr -g rerun/rerun_struct${1}_coord${2}.log >& rerun/rerun_struct${1}_coord${2}.out
        }

        fn0_vanilla () {
            [ ! -f rerun/rerun_struct${1}_coord${2}.edr ] && __GMXBIN__ mdrun -rerun lambda${2}/lambda.${ext} -s lambda${1}/lambda_nocont.tpr -e rerun/rerun_struct${1}_coord${2}.edr -g rerun/rerun_struct${1}_coord${2}.log >& rerun/rerun_struct${1}_coord${2}.out
        }

        input_n_jobs=__N_JOBS__

        if [[ "x${input_n_jobs}" == "x-1" ]]
        then
            if [[ "x$SLURM_CPUS_ON_NODE" != "x" ]]
            then
                # This is a slurm scheduler
                if [[ "x$CUDA_VISIBLE_DEVICES" != "x" ]]
                then
                    # There are GPU assigned to this job. Use a job per GPU.
                    gpu_array=(${CUDA_VISIBLE_DEVICES//,/ })
                    n_jobs=${#gpu_array[@]}
                    n_cpus=$(echo "print(int($SLURM_CPUS_ON_NODE / $n_jobs))" | python3 -u)
                    fn() { fn0_cpu_gpu "$@" ; }
                else
                    # There are no GPU. Use 2 CPU per run
                    n_cpus=2
                    n_jobs=$(echo "print(int($SLURM_CPUS_ON_NODE / $n_cpus))" | python3 -u)
                    fn() { fn0_cpu "$@" ; }
                fi
            elif [[ "x$PBS_NODEFILE" != "x" ]]
            then
                # This is a PBS scheduler
                tot_cpus=$(cat $PBS_NODEFILE | wc -l)
                if [[ "x$PBS_GPUFILE" != "x" ]]
                then
                    # There are GPU assigned to this job. Use a job per GPU.
                    CUDA_VISIBLE_DEVICES=$(cat $PBS_GPUFILE | awk -F"-gpu" '{ printf A$2;A=","}')
                    gpu_array=(${CUDA_VISIBLE_DEVICES//,/ })
                    n_jobs=${#gpu_array[@]}
                    n_cpus=$(echo "print(int($tot_cpus / $n_jobs))" | python3 -u)
                    fn() { fn0_cpu_gpu "$@" ; }
                else
                    # There are no GPU. Use 2 CPU per run
                    n_cpus=2
                    n_jobs=$(echo "print(int($tot_cpus / $n_cpus))" | python3 -u)
                    fn() { fn0_cpu "$@" ; }
                fi
            else
                # This is another scheduler or Bash, fall back to 1 job
                n_jobs=1
                fn() { fn0_vanilla "$@" ; }
            fi
        else
            n_jobs=${input_n_jobs}
            fn() { fn0_vanilla "$@" ; }
        fi
        export -f fn
        export -f fn0_cpu_gpu
        export -f fn0_cpu
        export -f fn0_vanilla
        export n_cpus
        export ext
        export CUDA_VISIBLE_DEVICES

        parallel --version > /dev/null 2>&1 || { echo "[ERROR] GNU parallel not found in $PATH, cannot continue" && exit -1; }

        n_lambda=$(ls lambda*/lambda.tpr | wc -l)
        lambda_exp=$(seq 0 $(echo ${n_lambda}-1 | bc))
        echo $n_jobs
        parallel --env fn0_cpu_gpu --env fn0_cpu --env fn0_vanilla --env fn --env n_cpus --env CUDA_VISIBLE_DEVICES --env ext --joblog rerun_joblog_$(date "+%H%M%S_%d%m%Y").log --jobs $n_jobs "fn {1} {2} {%}" ::: ${lambda_exp} ::: ${lambda_exp}
        [ $(ls rerun/rerun_struct*_coord*.edr | wc -l) -ne $(echo ${n_lambda}^2 | bc) ] && { echo "Failed to during rerun step on line $LINENO"; exit 1; }

collect = cd rerun || { echo "Could not cd into rerun directory, cannot continue" && exit -1; }

          input_n_jobs=__N_JOBS__

          if [[ "x${input_n_jobs}" == "-1" ]]
          then
              collect_n_jobs=1
          else
              collect_n_jobs=${input_n_jobs}
          fi

          parallel --jobs $collect_n_jobs '[ ! -f lambda{}.xvg ] && printf "pV\n\n" | __GMXBIN__ energy -f ../lambda{}/lambda.edr -o lambda{}.xvg' ::: ${lambda_exp}
          parallel --jobs $collect_n_jobs '[ ! -f {.}.xvg ] && printf "Potential\n\n" | __GMXBIN__ energy -f {} -o {.}.xvg' ::: *.edr

          [ $(ls rerun_struct*_coord*.xvg lambda*.xvg | wc -l) -ne $(echo ${n_lambda}^2 + ${n_lambda} | bc) ] && "Failed during collect step on line $LINENO"

          __COLLECT_BIN__

analysis = cd ..

           if ( [ $(ls lambda*/lambda_pbc.xtc 2>/dev/null | wc -l) -ne ${n_lambda} ] && [ $(ls lambda*/lambda_pbcfit.xtc 2>/dev/null | wc -l) -ne ${n_lambda} ] )
           then
               parallel --jobs=1 '( [ ! -f "{.}_pbcfit.xtc" ] && [ ! -f "{.}_pbc.xtc" ] ) && printf "System\n" | __GMXBIN__ trjconv -pbc mol -f {} -s {.}.tpr -o {.}_pbc.xtc -n ../__INDEX__ -skip __SKIP_FRAMES__ >> {//}/analysis.log 2>&1' ::: $(ls lambda*/lambda.${ext})
               [ $(ls lambda*/lambda_pbc.xtc 2>/dev/null | wc -l) -ne ${n_lambda} ] && { echo "Failed to run trjconv pbc on line $LINENO"; exit 1; }
           fi

           if [ $(ls lambda*/lambda_pbcfit.xtc 2>/dev/null | wc -l) -ne ${n_lambda} ]
           then
               parallel --jobs=$n_jobs '[ ! -f {.}_pbcfit.xtc ] && printf "C-alpha\nC-alpha\nSystem\n" | __GMXBIN__ trjconv -fit rot+trans -center -f {//}/lambda_pbc.xtc -s {} -o {//}/lambda_pbcfit.xtc -n ../__INDEX__ >> {//}/analysis.log 2>&1' ::: $(ls lambda*/lambda.tpr)
               [ $(ls lambda*/lambda_pbcfit.xtc | wc -l) -ne ${n_lambda} ] && { echo "Failed to run trjconv fit on line $LINENO"; exit 1; }
               rm lambda*/lambda_pbc.xtc
           fi

           trajectory_files=$(ls lambda*/lambda.tpr)
           ( parallel --jobs=$n_jobs 'printf "__LIG_GROUP__\n" | __GMXBIN__ rms -f {.}_pbcfit.xtc -s {} -n ../__INDEX__ -fit none -o analysis/{//}_ligrmsd.xvg >> {//}/analysis.log 2>&1' ::: ${trajectory_files} ) || { echo "Failed to calculate ligand RMSD on line $LINENO"; }
           ( parallel --jobs=$n_jobs 'printf  "Protein\n" | __GMXBIN__ rms -f {.}_pbcfit.xtc -s {} -n ../__INDEX__ -fit none -o analysis/{//}_protrmsd.xvg >> {//}/analysis.log 2>&1' ::: ${trajectory_files} ) || { echo "Failed to calculate protein RMSD on line $LINENO"; }
           ( parallel --jobs=$n_jobs 'printf "__LIG_GROUP__\n" | __GMXBIN__ rmsf -f {.}_pbcfit.xtc -s {} -n ../__INDEX__ -nofit -o analysis/{//}_ligrmsf.xvg >> {//}/analysis.log 2>&1' ::: ${trajectory_files} ) || { echo "Failed to calculate Ligand RMSF on line $LINENO"; }
           ( parallel --jobs=$n_jobs 'printf  "Protein\n" | __GMXBIN__ rmsf -f {.}_pbcfit.xtc -s {} -n ../__INDEX__ -res -nofit -o analysis/{//}_protrmsf.xvg >> {//}/analysis.log 2>&1' ::: ${trajectory_files} ) || { echo "Failed to calculate protein RMSF on line $LINENO"; }
           ( parallel --jobs=$n_jobs '__GMXBIN__ distance -select \"com of group \\"__LIG_GROUP__\\" plus com of group \\"Protein\\"\" -f {/.}_pbcfit.xtc -s {} -n ../__INDEX__ -oall {//}_protligdist.xvg >> {//}/analysis.log 2>&1' ::: ${trajectory_files} ) || { echo "Failed to calculate distance between ligand and protein on line $LINENO"; }
           ( parallel --jobs=$n_jobs '__GMXBIN__ sasa -f {.}_pbcfit.xtc -s {} -n ../__INDEX__ -surface \"Protein\" -o analysis/{//}_protein_sasa.xvg >> {//}/analysis.log 2>&1' ::: ${trajectory_files} ) || { echo "Failed to run SASA calculation on line $LINENO"; }

pack = tar --create --gzip --file __PACKED_FILE__ */{water,protein}/md/rerun/{*.pkl,*.xvg,*.edr} \
                                                  */{water,protein}/md/lambda*/lambda{_pbcfit.xtc,_nocont.tpr,.tpr,.log,.edr} \
                                                  */{water,protein}/md/analysis \
                                                  *.pkl


[default]
selfextracting = #! /bin/bash
                 echo ""
                 echo "Extracting perturbation input data"
                 echo ""

                 ARCHIVE=`awk '/^__ARCHIVE_BELOW__/ {print NR + 1; exit 0; }' $0`

                 tail -n+$ARCHIVE $0 | tar xz

                 echo "All done. Run 'bash runall.sh' to submit."
                 echo ""

                 exit 0

                 __ARCHIVE_BELOW__

scripts_names = equilibrate, run, rerun, collect, analysis, pack

shebang = #!/bin/bash
